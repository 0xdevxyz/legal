"""Advanced Text Analysis for Compliance Documents"""

import re
from typing import Dict, List, Any, Tuple, Optional
from collections import Counter
import math

class ComplianceTextAnalyzer:
    """Advanced text analysis for compliance documents"""
    
    def __init__(self):
        self.compliance_terms = {
            'gdpr_terms': [
                'personal data', 'processing', 'controller', 'processor', 'consent',
                'legitimate interest', 'data subject', 'supervisory authority'
            ],
            'privacy_terms': [
                'privacy policy', 'data collection', 'information sharing', 'third party',
                'cookies', 'tracking', 'opt-out', 'unsubscribe'
            ],
            'legal_terms': [
                'terms of service', 'agreement', 'liability', 'warranty', 'jurisdiction',
                'applicable law', 'dispute resolution'
            ]
        }
        
    def analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """Analyze document structure and organization"""
        
        sections = self._identify_sections(text)
        headings = self._extract_headings(text)
        
        analysis = {
            'total_length': len(text),
            'word_count': len(text.split()),
            'paragraph_count': len(text.split('\n\n')),
            'sections': sections,
            'headings': headings,
            'structure_score': self._calculate_structure_score(sections, headings),
            'readability_metrics': self._calculate_readability_metrics(text)
        }
        
        return analysis
    
    def extract_key_information(self, text: str) -> Dict[str, Any]:
        """Extract key compliance information from text"""
        
        return {
            'contact_information': self._extract_contact_info(text),
            'data_categories': self._extract_data_categories(text),
            'processing_activities': self._extract_processing_activities(text),
            'legal_bases': self._extract_legal_bases(text),
            'retention_periods': self._extract_retention_periods(text),
            'third_party_services': self._extract_third_party_services(text),
            'user_rights_mentioned': self._extract_user_rights(text),
            'compliance_obligations': self._extract_compliance_obligations(text)
        }
    
    def assess_language_quality(self, text: str) -> Dict[str, Any]:
        """Assess language quality and clarity"""
        
        sentences = self._split_sentences(text)
        words = text.split()
        
        return {
            'average_sentence_length': len(words) / max(1, len(sentences)),
            'vocabulary_diversity': self._calculate_vocabulary_diversity(words),
            'passive_voice_ratio': self._calculate_passive_voice_ratio(sentences),
            'jargon_density': self._calculate_jargon_density(text),
            'clarity_score': self._calculate_clarity_score(text),
            'professional_tone_score': self._assess_professional_tone(text)
        }
    
    def identify_compliance_gaps(self, text: str, compliance_type: str = 'gdpr') -> List[Dict[str, Any]]:
        """Identify potential compliance gaps in the document"""
        
        gaps = []
        
        if compliance_type.lower() == 'gdpr':
            gaps.extend(self._check_gdpr_requirements(text))
        elif compliance_type.lower() == 'ccpa':
            gaps.extend(self._check_ccpa_requirements(text))
        elif compliance_type.lower() == 'accessibility':
            gaps.extend(self._check_accessibility_requirements(text))
        
        return gaps
    
    def _identify_sections(self, text: str) -> List[Dict[str, Any]]:
        \"\"\"Identify document sections\"\"\"\n        \n        sections = []\n        \n        # Look for numbered sections\n        numbered_pattern = r'^(\\d+\\.\\s+[^\\n]+)'\n        numbered_matches = re.finditer(numbered_pattern, text, re.MULTILINE)\n        \n        for match in numbered_matches:\n            sections.append({\n                'type': 'numbered',\n                'title': match.group(1).strip(),\n                'position': match.start()\n            })\n        \n        # Look for capitalized sections\n        caps_pattern = r'^([A-Z][A-Z\\s]{3,})$'\n        caps_matches = re.finditer(caps_pattern, text, re.MULTILINE)\n        \n        for match in caps_matches:\n            sections.append({\n                'type': 'capitalized',\n                'title': match.group(1).strip(),\n                'position': match.start()\n            })\n        \n        return sorted(sections, key=lambda x: x['position'])\n    \n    def _extract_headings(self, text: str) -> List[str]:\n        \"\"\"Extract various types of headings\"\"\"\n        \n        headings = []\n        \n        # HTML headings\n        html_pattern = r'<h[1-6][^>]*>([^<]+)</h[1-6]>'\n        html_matches = re.findall(html_pattern, text, re.IGNORECASE)\n        headings.extend(html_matches)\n        \n        # Markdown headings\n        md_pattern = r'^#+\\s+(.+)$'\n        md_matches = re.findall(md_pattern, text, re.MULTILINE)\n        headings.extend(md_matches)\n        \n        return headings\n    \n    def _calculate_structure_score(self, sections: List[Dict], headings: List[str]) -> float:\n        \"\"\"Calculate document structure quality score\"\"\"\n        \n        score = 50  # Base score\n        \n        # Bonus for having sections\n        if sections:\n            score += min(30, len(sections) * 5)\n        \n        # Bonus for having headings\n        if headings:\n            score += min(20, len(headings) * 3)\n        \n        return min(100, score)\n    \n    def _calculate_readability_metrics(self, text: str) -> Dict[str, float]:\n        \"\"\"Calculate various readability metrics\"\"\"\n        \n        sentences = self._split_sentences(text)\n        words = text.split()\n        syllables = self._count_total_syllables(text)\n        \n        if not sentences or not words:\n            return {'flesch_kincaid': 0, 'flesch_reading_ease': 0, 'gunning_fog': 0}\n        \n        avg_sentence_length = len(words) / len(sentences)\n        avg_syllables_per_word = syllables / len(words)\n        \n        # Flesch-Kincaid Grade Level\n        fk_grade = 0.39 * avg_sentence_length + 11.8 * avg_syllables_per_word - 15.59\n        \n        # Flesch Reading Ease\n        fre_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n        \n        # Gunning Fog Index\n        complex_words = sum(1 for word in words if self._count_syllables(word) >= 3)\n        complex_word_ratio = complex_words / len(words) if words else 0\n        gunning_fog = 0.4 * (avg_sentence_length + 100 * complex_word_ratio)\n        \n        return {\n            'flesch_kincaid_grade': max(0, fk_grade),\n            'flesch_reading_ease': max(0, min(100, fre_score)),\n            'gunning_fog_index': max(0, gunning_fog)\n        }\n    \n    def _extract_contact_info(self, text: str) -> Dict[str, List[str]]:\n        \"\"\"Extract contact information\"\"\"\n        \n        return {\n            'email_addresses': re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text),\n            'phone_numbers': re.findall(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', text),\n            'addresses': self._extract_addresses(text)\n        }\n    \n    def _extract_addresses(self, text: str) -> List[str]:\n        \"\"\"Extract physical addresses\"\"\"\n        \n        # Simple address pattern matching\n        address_patterns = [\n            r'\\d+\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln)\\b[^\\n]*',\n            r'\\b[A-Za-z\\s]+,\\s*[A-Za-z\\s]+\\s+\\d{5}\\b'\n        ]\n        \n        addresses = []\n        for pattern in address_patterns:\n            matches = re.findall(pattern, text)\n            addresses.extend(matches)\n        \n        return addresses\n    \n    def _extract_data_categories(self, text: str) -> List[str]:\n        \"\"\"Extract mentioned data categories\"\"\"\n        \n        data_categories = [\n            'personal information', 'contact information', 'payment information',\n            'device information', 'location data', 'usage data', 'cookies',\n            'biometric data', 'financial information', 'health information'\n        ]\n        \n        found_categories = []\n        text_lower = text.lower()\n        \n        for category in data_categories:\n            if category in text_lower:\n                found_categories.append(category)\n        \n        return found_categories\n    \n    def _extract_processing_activities(self, text: str) -> List[str]:\n        \"\"\"Extract data processing activities\"\"\"\n        \n        activities = [\n            'collect', 'store', 'process', 'analyze', 'share', 'transfer',\n            'delete', 'retain', 'aggregate', 'combine', 'disclose'\n        ]\n        \n        found_activities = []\n        text_lower = text.lower()\n        \n        for activity in activities:\n            if activity in text_lower:\n                found_activities.append(activity)\n        \n        return found_activities\n    \n    def _extract_legal_bases(self, text: str) -> List[str]:\n        \"\"\"Extract legal bases for processing\"\"\"\n        \n        legal_bases = [\n            'consent', 'contract', 'legal obligation', 'vital interests',\n            'public task', 'legitimate interests', 'legitimate interest'\n        ]\n        \n        found_bases = []\n        text_lower = text.lower()\n        \n        for basis in legal_bases:\n            if basis in text_lower:\n                found_bases.append(basis)\n        \n        return found_bases\n    \n    def _extract_retention_periods(self, text: str) -> List[str]:\n        \"\"\"Extract data retention periods\"\"\"\n        \n        # Pattern for time periods\n        time_patterns = [\n            r'\\d+\\s*(?:day|week|month|year)s?',\n            r'(?:as long as|until|for the duration of)[^.!?]*',\n            r'(?:permanently|indefinitely|forever)'\n        ]\n        \n        retention_periods = []\n        for pattern in time_patterns:\n            matches = re.finditer(pattern, text.lower())\n            for match in matches:\n                # Get context around the match\n                start = max(0, match.start() - 30)\n                end = min(len(text), match.end() + 30)\n                context = text[start:end].strip()\n                retention_periods.append(context)\n        \n        return retention_periods\n    \n    def _extract_third_party_services(self, text: str) -> List[str]:\n        \"\"\"Extract third-party services mentioned\"\"\"\n        \n        common_services = [\n            'google analytics', 'facebook', 'twitter', 'linkedin', 'instagram',\n            'mailchimp', 'salesforce', 'hubspot', 'stripe', 'paypal',\n            'amazon', 'microsoft', 'adobe', 'cloudflare'\n        ]\n        \n        found_services = []\n        text_lower = text.lower()\n        \n        for service in common_services:\n            if service in text_lower:\n                found_services.append(service)\n        \n        return found_services\n    \n    def _extract_user_rights(self, text: str) -> List[str]:\n        \"\"\"Extract user rights mentioned in the document\"\"\"\n        \n        rights = [\n            'right to access', 'right to rectification', 'right to erasure',\n            'right to portability', 'right to object', 'right to restrict processing',\n            'withdraw consent', 'opt out', 'unsubscribe', 'delete account'\n        ]\n        \n        found_rights = []\n        text_lower = text.lower()\n        \n        for right in rights:\n            if right in text_lower:\n                found_rights.append(right)\n        \n        return found_rights\n    \n    def _extract_compliance_obligations(self, text: str) -> List[str]:\n        \"\"\"Extract compliance obligations mentioned\"\"\"\n        \n        obligations = [\n            'gdpr', 'ccpa', 'hipaa', 'coppa', 'ferpa', 'sox', 'pci dss',\n            'data protection', 'privacy shield', 'standard contractual clauses'\n        ]\n        \n        found_obligations = []\n        text_lower = text.lower()\n        \n        for obligation in obligations:\n            if obligation in text_lower:\n                found_obligations.append(obligation)\n        \n        return found_obligations\n    \n    def _split_sentences(self, text: str) -> List[str]:\n        \"\"\"Split text into sentences\"\"\"\n        \n        # Simple sentence splitting\n        sentences = re.split(r'[.!?]+', text)\n        return [s.strip() for s in sentences if s.strip()]\n    \n    def _count_syllables(self, word: str) -> int:\n        \"\"\"Count syllables in a word\"\"\"\n        \n        word = word.lower().strip()\n        if not word:\n            return 0\n        \n        vowels = 'aeiouy'\n        syllable_count = 0\n        prev_was_vowel = False\n        \n        for char in word:\n            if char in vowels:\n                if not prev_was_vowel:\n                    syllable_count += 1\n                prev_was_vowel = True\n            else:\n                prev_was_vowel = False\n        \n        # Adjust for silent e\n        if word.endswith('e'):\n            syllable_count = max(1, syllable_count - 1)\n        \n        return max(1, syllable_count)\n    \n    def _count_total_syllables(self, text: str) -> int:\n        \"\"\"Count total syllables in text\"\"\"\n        \n        words = re.findall(r'\\b\\w+\\b', text.lower())\n        return sum(self._count_syllables(word) for word in words)\n    \n    def _calculate_vocabulary_diversity(self, words: List[str]) -> float:\n        \"\"\"Calculate vocabulary diversity (Type-Token Ratio)\"\"\"\n        \n        if not words:\n            return 0.0\n        \n        unique_words = set(word.lower() for word in words)\n        return len(unique_words) / len(words)\n    \n    def _calculate_passive_voice_ratio(self, sentences: List[str]) -> float:\n        \"\"\"Calculate ratio of passive voice sentences\"\"\"\n        \n        if not sentences:\n            return 0.0\n        \n        passive_indicators = ['was', 'were', 'been', 'being', 'be']\n        past_participle_endings = ['ed', 'en', 'n', 't']\n        \n        passive_count = 0\n        for sentence in sentences:\n            words = sentence.lower().split()\n            has_passive_indicator = any(indicator in words for indicator in passive_indicators)\n            has_past_participle = any(word.endswith(ending) for word in words for ending in past_participle_endings)\n            \n            if has_passive_indicator and has_past_participle:\n                passive_count += 1\n        \n        return passive_count / len(sentences)\n    \n    def _calculate_jargon_density(self, text: str) -> float:\n        \"\"\"Calculate density of technical/legal jargon\"\"\"\n        \n        jargon_terms = [\n            'aforementioned', 'hereinafter', 'pursuant', 'whereby', 'heretofore',\n            'notwithstanding', 'therein', 'thereof', 'whereas', 'hereunder'\n        ]\n        \n        words = text.lower().split()\n        if not words:\n            return 0.0\n        \n        jargon_count = sum(1 for word in words if any(jargon in word for jargon in jargon_terms))\n        return jargon_count / len(words)\n    \n    def _calculate_clarity_score(self, text: str) -> float:\n        \"\"\"Calculate overall clarity score\"\"\"\n        \n        readability = self._calculate_readability_metrics(text)\n        words = text.split()\n        sentences = self._split_sentences(text)\n        \n        # Factors that contribute to clarity\n        factors = {\n            'readability': min(100, readability.get('flesch_reading_ease', 0)),\n            'sentence_length': max(0, 100 - (len(words) / max(1, len(sentences))) * 2),\n            'jargon_penalty': max(0, 100 - (self._calculate_jargon_density(text) * 200)),\n            'vocabulary_diversity': self._calculate_vocabulary_diversity(words) * 100\n        }\n        \n        # Weighted average\n        weights = {'readability': 0.3, 'sentence_length': 0.3, 'jargon_penalty': 0.2, 'vocabulary_diversity': 0.2}\n        clarity_score = sum(factors[factor] * weights[factor] for factor in factors)\n        \n        return min(100, max(0, clarity_score))\n    \n    def _assess_professional_tone(self, text: str) -> float:\n        \"\"\"Assess professional tone of the document\"\"\"\n        \n        professional_indicators = [\n            'we', 'our', 'company', 'organization', 'policy', 'procedure',\n            'commitment', 'ensure', 'maintain', 'protect', 'comply'\n        ]\n        \n        unprofessional_indicators = [\n            'awesome', 'cool', 'stuff', 'thing', 'guy', 'dude'\n        ]\n        \n        words = text.lower().split()\n        if not words:\n            return 50.0\n        \n        professional_count = sum(1 for word in words if word in professional_indicators)\n        unprofessional_count = sum(1 for word in words if word in unprofessional_indicators)\n        \n        professional_ratio = professional_count / len(words)\n        unprofessional_ratio = unprofessional_count / len(words)\n        \n        # Score from 0-100\n        score = 50 + (professional_ratio * 200) - (unprofessional_ratio * 300)\n        return min(100, max(0, score))\n    \n    def _check_gdpr_requirements(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"Check for GDPR compliance requirements\"\"\"\n        \n        gaps = []\n        text_lower = text.lower()\n        \n        gdpr_requirements = {\n            'data_controller_identity': ['controller', 'data controller', 'responsible'],\n            'processing_purposes': ['purpose', 'reason for processing'],\n            'legal_basis': ['legal basis', 'lawful basis'],\n            'legitimate_interests': ['legitimate interest', 'legitimate interests'],\n            'recipient_categories': ['recipient', 'third party', 'share'],\n            'international_transfers': ['transfer', 'international', 'third country'],\n            'retention_period': ['retention', 'keep', 'store', 'delete'],\n            'data_subject_rights': ['rights', 'access', 'rectification', 'erasure'],\n            'withdrawal_of_consent': ['withdraw', 'consent'],\n            'complaint_authority': ['supervisory authority', 'data protection authority', 'complaint'],\n            'automated_decision_making': ['automated', 'profiling', 'algorithm'],\n            'dpo_contact': ['data protection officer', 'dpo']\n        }\n        \n        for requirement, keywords in gdpr_requirements.items():\n            if not any(keyword in text_lower for keyword in keywords):\n                gaps.append({\n                    'requirement': requirement,\n                    'description': f'Missing information about {requirement.replace(\"_\", \" \")}',\n                    'severity': 'high' if requirement in ['legal_basis', 'processing_purposes'] else 'medium',\n                    'compliance_type': 'gdpr'\n                })\n        \n        return gaps\n    \n    def _check_ccpa_requirements(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"Check for CCPA compliance requirements\"\"\"\n        \n        gaps = []\n        text_lower = text.lower()\n        \n        ccpa_requirements = {\n            'personal_information_categories': ['categories of personal information', 'personal information'],\n            'sources_of_information': ['source', 'collect'],\n            'business_purposes': ['business purpose', 'commercial purpose'],\n            'third_party_disclosure': ['disclose', 'sell', 'share'],\n            'consumer_rights': ['right to know', 'right to delete', 'right to opt-out'],\n            'do_not_sell': ['do not sell', 'opt-out'],\n            'non_discrimination': ['discriminate', 'different treatment']\n        }\n        \n        for requirement, keywords in ccpa_requirements.items():\n            if not any(keyword in text_lower for keyword in keywords):\n                gaps.append({\n                    'requirement': requirement,\n                    'description': f'Missing CCPA information about {requirement.replace(\"_\", \" \")}',\n                    'severity': 'high' if requirement in ['consumer_rights', 'do_not_sell'] else 'medium',\n                    'compliance_type': 'ccpa'\n                })\n        \n        return gaps\n    \n    def _check_accessibility_requirements(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"Check for accessibility requirements in content\"\"\"\n        \n        gaps = []\n        \n        # This would be used for analyzing accessibility policy text\n        accessibility_keywords = ['accessibility', 'wcag', 'ada', 'disability', 'assistive technology']\n        \n        if not any(keyword in text.lower() for keyword in accessibility_keywords):\n            gaps.append({\n                'requirement': 'accessibility_statement',\n                'description': 'No accessibility statement or policy found',\n                'severity': 'medium',\n                'compliance_type': 'accessibility'\n            })\n        \n        return gaps