"""
Deep Content Analyzer
Analysiert tats√§chlichen Seiteninhalt statt nur Link-Existenz

Features:
- Crawlt und analysiert Impressum/Datenschutz-Seiten
- Pattern-basierte Erkennung (schnell, 90% F√§lle)
- KI-Validierung bei Unsicherheit (10% F√§lle)
- Qualit√§tsbewertung der Inhalte
"""

import re
import logging
from typing import Dict, List, Any, Optional, Tuple
from bs4 import BeautifulSoup
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger(__name__)


class ContentQuality(Enum):
    """Qualit√§tsbewertung des Inhalts"""
    EXCELLENT = "excellent"  # Alle Pflichtangaben vorhanden, hochwertig
    GOOD = "good"            # Alle Pflichtangaben vorhanden
    ACCEPTABLE = "acceptable" # Meiste Pflichtangaben vorhanden
    POOR = "poor"            # Einige Pflichtangaben fehlen
    INSUFFICIENT = "insufficient"  # Viele Pflichtangaben fehlen


@dataclass
class ContentValidation:
    """Ergebnis einer Content-Validierung"""
    field_name: str
    found: bool
    confidence: float  # 0.0 - 1.0
    extracted_value: Optional[str] = None
    location: Optional[str] = None  # CSS-Selector oder Text-Position
    quality_score: float = 0.0  # 0-100


@dataclass
class DeepAnalysisResult:
    """Ergebnis einer Deep-Content-Analyse"""
    url: str
    page_type: str  # "impressum", "datenschutz", etc.
    overall_quality: ContentQuality
    validations: List[ContentValidation] = field(default_factory=list)
    missing_fields: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    confidence: float = 0.0  # Gesamtvertrauen in die Analyse


class DeepContentAnalyzer:
    """
    Deep Content Analyzer f√ºr Compliance-Pr√ºfung
    
    Pr√ºft tats√§chlichen Inhalt von Seiten statt nur Link-Existenz
    """
    
    def __init__(self):
        """Initialisiert Analyzer mit Pattern-Definitionen"""
        
        # ========================================================================
        # IMPRESSUM PATTERNS
        # ========================================================================
        
        self.impressum_patterns = {
            "firmenname": {
                "patterns": [
                    # Explizite Markierungen
                    r"(?:firma|firmenname|company|unternehmen):\s*([A-Z√Ñ√ñ√úa-z√§√∂√º\s&.-]+(?:GmbH|AG|UG|e\.K\.|KG)?)",
                    # Nach bekannten Rechtsformen
                    r"([A-Z√Ñ√ñ√úa-z√§√∂√º\s&.-]+\s+(?:GmbH|AG|UG|e\.K\.|KG|OHG|PartG|Einzelunternehmen))",
                    # Strukturierte Daten
                    r"<[^>]*(?:itemprop|property)=['\"]name['\"][^>]*>([^<]+)<",
                ],
                "required": True,
                "min_confidence": 0.7
            },
            
            "adresse": {
                "patterns": [
                    # Stra√üe + Hausnummer + PLZ + Ort
                    r"([A-Z√Ñ√ñ√úa-z√§√∂√º√ü][a-z√§√∂√º√ü]+(?:stra√üe|str\.|weg|platz|allee)[\s,]+\d+[a-z]?[\s,]+\d{5}[\s,]+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)",
                    # PLZ + Ort + Stra√üe (umgekehrte Reihenfolge)
                    r"(\d{5}[\s,]+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+[\s,]+[A-Z√Ñ√ñ√úa-z√§√∂√º√ü][a-z√§√∂√º√ü]+(?:stra√üe|str\.|weg|platz)[\s,]+\d+)",
                    # Address-Tag
                    r"<address[^>]*>([^<]+)</address>",
                    # Schema.org markup
                    r"itemprop=['\"]streetAddress['\"][^>]*>([^<]+)",
                ],
                "required": True,
                "min_confidence": 0.8
            },
            
            "plz_ort": {
                "patterns": [
                    r"(\d{5}[\s,]+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü-]+)",
                    r"itemprop=['\"]postalCode['\"][^>]*>(\d{5})",
                ],
                "required": True,
                "min_confidence": 0.9
            },
            
            "email": {
                "patterns": [
                    r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})",
                    r"mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})",
                    r"itemprop=['\"]email['\"][^>]*>([^<]+@[^<]+)",
                ],
                "required": True,
                "min_confidence": 0.95
            },
            
            "telefon": {
                "patterns": [
                    # Deutsch: +49, 0, (0)
                    r"(?:tel:|telefon:|phone:)?\s*(\+49[\s\-./]*\(?0?\)?[\s\-./]*\d{2,5}[\s\-./]*\d{3,9})",
                    r"(\(?\+?49\)?[\s\-./]*\(?\d{2,5}\)?[\s\-./]*\d{3,9})",
                    r"(0\d{2,5}[\s\-./]*\d{3,9})",
                    r"itemprop=['\"]telephone['\"][^>]*>([^<]+)",
                ],
                "required": True,
                "min_confidence": 0.85
            },
            
            "handelsregister": {
                "patterns": [
                    r"(?:handelsregister|amtsgericht|hrb|hra|registergericht)[\s:]*([A-Z][a-z√§√∂√º√ü]+[\s,]+(?:HRB|HRA)\s*\d+)",
                    r"((?:HRB|HRA)\s*\d+[\s,]+AG\s+[A-Z][a-z√§√∂√º√ü]+)",
                ],
                "required": False,  # Nur f√ºr Unternehmen
                "min_confidence": 0.8
            },
            
            "ust_id": {
                "patterns": [
                    r"(?:umsatzsteuer-id|ust-id|vat|uid|ust\.?-?id\.?)[\s:]*([A-Z]{2}\s*\d{8,12})",
                    r"\b(DE\d{9})\b",
                ],
                "required": False,  # Optional
                "min_confidence": 0.95
            },
            
            "geschaeftsfuehrer": {
                "patterns": [
                    r"(?:gesch√§ftsf√ºhrer|gesch√§ftsf√ºhrung|ceo|managing director)[\s:]+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+\s+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)",
                ],
                "required": False,
                "min_confidence": 0.7
            }
        }
        
        # ========================================================================
        # DATENSCHUTZ PATTERNS
        # ========================================================================
        
        self.datenschutz_patterns = {
            "verantwortlicher": {
                "patterns": [
                    r"(?:verantwortlich|verantwortlicher|data controller|controller)[\s:]+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü\s&.-]+)",
                    r"(?:im\s+sinne\s+(?:der|des)\s+dsgvo|nach\s+art\.\s*13|gem√§√ü\s+art\.\s*13)[\s:]+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü\s]+)",
                ],
                "required": True,
                "min_confidence": 0.75
            },
            
            "zwecke": {
                "patterns": [
                    r"(?:zweck|zwecke|purpose|purposes)\s+(?:der\s+)?(?:datenverarbeitung|verarbeitung|processing)[\s:]+([^.]{20,200})",
                    r"(?:wir\s+(?:verarbeiten|nutzen|verwenden|erheben)\s+(?:ihre\s+)?(?:personen)?daten\s+(?:zu|f√ºr))[\s:]+([^.]{20,200})",
                ],
                "required": True,
                "min_confidence": 0.7
            },
            
            "rechtsgrundlage": {
                "patterns": [
                    r"(?:rechtsgrundlage|legal basis)[\s:]+([^.]{20,300})",
                    r"(?:art\.?\s*6\s+abs\.?\s*1|artikel\s+6)[\s:]+([^.]{10,200})",
                ],
                "required": True,
                "min_confidence": 0.75
            },
            
            "speicherdauer": {
                "patterns": [
                    r"(?:speicherdauer|aufbewahrungsfrist|retention period|storage duration)[\s:]+([^.]{10,200})",
                    r"(?:wir\s+speichern\s+(?:ihre\s+)?daten\s+(?:f√ºr|bis))[\s:]+([^.]{10,150})",
                ],
                "required": True,
                "min_confidence": 0.7
            },
            
            "betroffenenrechte": {
                "patterns": [
                    r"(?:ihre\s+)?(?:rechte|betroffenenrechte|rights)[\s:]+([^.]{30,500})",
                    r"(?:auskunft|berichtigung|l√∂schung|widerruf|widerspruch)[\s,]+(?:berichtigung|l√∂schung|widerruf|widerspruch)",
                ],
                "required": True,
                "min_confidence": 0.65
            },
            
            "beschwerderecht": {
                "patterns": [
                    r"(?:beschwerderecht|right to lodge a complaint|aufsichtsbeh√∂rde|supervisory authority)[\s:]+([^.]{20,300})",
                ],
                "required": True,
                "min_confidence": 0.7
            },
            
            "datenschutzbeauftragter": {
                "patterns": [
                    r"(?:datenschutzbeauftragter|data protection officer|dpo)[\s:]+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü\s]+)",
                ],
                "required": False,  # Nur wenn ben√∂tigt
                "min_confidence": 0.8
            },
            
            "drittland": {
                "patterns": [
                    r"(?:drittland|third country|au√üerhalb der eu|outside the eu)",
                ],
                "required": False,
                "min_confidence": 0.8
            },
            
            "ssl_verschluesselung": {
                "patterns": [
                    r"(?:ssl|tls|verschl√ºsselung|encryption|https)",
                ],
                "required": False,
                "min_confidence": 0.9
            }
        }
    
    # ========================================================================
    # MAIN ANALYSIS METHODS
    # ========================================================================
    
    async def analyze_impressum_page(
        self,
        url: str,
        html: str
    ) -> DeepAnalysisResult:
        """
        Analysiert Impressum-Seite im Detail
        
        Args:
            url: URL der Impressum-Seite
            html: HTML-Content der Seite
        
        Returns:
            DeepAnalysisResult mit Validierungen
        """
        logger.info(f"üîç Deep-Analyse: Impressum {url}")
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Entferne Script/Style-Tags
        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
            tag.decompose()
        
        # Extrahiere Text-Content
        text_content = soup.get_text(separator=' ', strip=True)
        
        # Validiere alle Felder
        validations = []
        for field_name, field_config in self.impressum_patterns.items():
            validation = self._validate_field(
                field_name,
                field_config,
                text_content,
                soup
            )
            validations.append(validation)
        
        # Berechne Gesamtqualit√§t
        overall_quality, missing_fields, warnings = self._calculate_quality(
            validations,
            self.impressum_patterns
        )
        
        # Gesamtvertrauen
        confidence = sum(v.confidence for v in validations) / len(validations) if validations else 0.0
        
        result = DeepAnalysisResult(
            url=url,
            page_type="impressum",
            overall_quality=overall_quality,
            validations=validations,
            missing_fields=missing_fields,
            warnings=warnings,
            confidence=confidence
        )
        
        logger.info(f"‚úÖ Impressum-Analyse: {overall_quality.value} (Confidence: {confidence:.2f})")
        
        return result
    
    async def analyze_datenschutz_page(
        self,
        url: str,
        html: str
    ) -> DeepAnalysisResult:
        """
        Analysiert Datenschutzerkl√§rung im Detail
        
        Args:
            url: URL der Datenschutz-Seite
            html: HTML-Content der Seite
        
        Returns:
            DeepAnalysisResult mit Validierungen
        """
        logger.info(f"üîç Deep-Analyse: Datenschutz {url}")
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Entferne Script/Style-Tags
        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
            tag.decompose()
        
        text_content = soup.get_text(separator=' ', strip=True)
        
        # Validiere alle Felder
        validations = []
        for field_name, field_config in self.datenschutz_patterns.items():
            validation = self._validate_field(
                field_name,
                field_config,
                text_content,
                soup
            )
            validations.append(validation)
        
        # Berechne Qualit√§t
        overall_quality, missing_fields, warnings = self._calculate_quality(
            validations,
            self.datenschutz_patterns
        )
        
        confidence = sum(v.confidence for v in validations) / len(validations) if validations else 0.0
        
        result = DeepAnalysisResult(
            url=url,
            page_type="datenschutz",
            overall_quality=overall_quality,
            validations=validations,
            missing_fields=missing_fields,
            warnings=warnings,
            confidence=confidence
        )
        
        logger.info(f"‚úÖ Datenschutz-Analyse: {overall_quality.value} (Confidence: {confidence:.2f})")
        
        return result
    
    # ========================================================================
    # VALIDATION LOGIC
    # ========================================================================
    
    def _validate_field(
        self,
        field_name: str,
        field_config: Dict[str, Any],
        text_content: str,
        soup: BeautifulSoup
    ) -> ContentValidation:
        """
        Validiert einzelnes Feld mit Pattern-Matching
        
        Returns:
            ContentValidation mit Ergebnis
        """
        patterns = field_config["patterns"]
        min_confidence = field_config["min_confidence"]
        
        best_match = None
        best_confidence = 0.0
        extracted_value = None
        
        # Teste alle Patterns
        for pattern in patterns:
            try:
                matches = re.finditer(pattern, text_content, re.IGNORECASE | re.MULTILINE)
                
                for match in matches:
                    # Extrahiere Value (erste Gruppe oder ganzer Match)
                    value = match.group(1) if match.groups() else match.group(0)
                    
                    # Confidence basierend auf Pattern-Typ und Match-Qualit√§t
                    confidence = self._calculate_match_confidence(
                        field_name,
                        pattern,
                        value,
                        text_content
                    )
                    
                    if confidence > best_confidence:
                        best_confidence = confidence
                        extracted_value = value.strip()
                        best_match = match
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Pattern-Fehler f√ºr {field_name}: {e}")
                continue
        
        # Resultat
        found = best_confidence >= min_confidence
        
        return ContentValidation(
            field_name=field_name,
            found=found,
            confidence=best_confidence,
            extracted_value=extracted_value,
            quality_score=best_confidence * 100
        )
    
    def _calculate_match_confidence(
        self,
        field_name: str,
        pattern: str,
        value: str,
        full_text: str
    ) -> float:
        """
        Berechnet Confidence-Score f√ºr ein Match
        
        Faktoren:
        - L√§nge des Matches (zu kurz/lang = suspicious)
        - Kontext (steht "Impressum" oder "Pflichtangaben" in der N√§he?)
        - Format-Validierung (z.B. Email-Format, Telefon-Format)
        """
        confidence = 0.5  # Base confidence
        
        # L√§ngen-Check
        if len(value) < 3:
            confidence *= 0.5  # Zu kurz
        elif 10 <= len(value) <= 200:
            confidence *= 1.3  # Optimale L√§nge
        elif len(value) > 500:
            confidence *= 0.7  # Zu lang
        
        # Kontext-Check (steht in der N√§he ein relevantes Keyword?)
        context_keywords = {
            "firmenname": ["firma", "unternehmen", "company"],
            "adresse": ["anschrift", "address", "sitz"],
            "email": ["e-mail", "email", "kontakt"],
            "telefon": ["tel", "phone", "fon"],
            "verantwortlicher": ["verantwortlich", "controller"],
        }
        
        if field_name in context_keywords:
            # Suche in ¬±100 Zeichen um das Match
            value_pos = full_text.find(value)
            if value_pos != -1:
                context = full_text[max(0, value_pos-100):min(len(full_text), value_pos+len(value)+100)].lower()
                
                for keyword in context_keywords[field_name]:
                    if keyword in context:
                        confidence *= 1.2
                        break
        
        # Format-Validierung
        if field_name == "email":
            # Erweiterte Email-Validierung
            if "@" in value and "." in value.split("@")[1]:
                confidence *= 1.3
        
        elif field_name == "ust_id":
            # USt-ID Format: DE + 9 Ziffern
            if re.match(r"^DE\d{9}$", value.replace(" ", "")):
                confidence *= 1.4
        
        # Cap at 1.0
        return min(1.0, confidence)
    
    def _calculate_quality(
        self,
        validations: List[ContentValidation],
        patterns_config: Dict[str, Dict]
    ) -> Tuple[ContentQuality, List[str], List[str]]:
        """
        Berechnet Gesamtqualit√§t basierend auf Validierungen
        
        Returns:
            (ContentQuality, missing_fields, warnings)
        """
        required_fields = [
            name for name, config in patterns_config.items()
            if config.get("required", False)
        ]
        
        found_required = [
            v for v in validations
            if v.field_name in required_fields and v.found
        ]
        
        missing_fields = [
            v.field_name for v in validations
            if v.field_name in required_fields and not v.found
        ]
        
        warnings = []
        
        # Qualit√§tsberechnung
        required_count = len(required_fields)
        found_count = len(found_required)
        
        if required_count == 0:
            return ContentQuality.EXCELLENT, missing_fields, warnings
        
        completion_rate = found_count / required_count
        
        # Durchschnittliche Confidence der gefundenen Felder
        avg_confidence = sum(v.confidence for v in found_required) / found_count if found_count > 0 else 0.0
        
        # Kombiniere Completion Rate und Confidence
        quality_score = (completion_rate * 0.7) + (avg_confidence * 0.3)
        
        # Bestimme Qualit√§tsstufe
        if quality_score >= 0.95 and avg_confidence >= 0.9:
            quality = ContentQuality.EXCELLENT
        elif quality_score >= 0.85:
            quality = ContentQuality.GOOD
        elif quality_score >= 0.7:
            quality = ContentQuality.ACCEPTABLE
            warnings.append(f"{len(missing_fields)} Pflichtangabe(n) fehlen")
        elif quality_score >= 0.5:
            quality = ContentQuality.POOR
            warnings.append(f"Mehrere wichtige Angaben fehlen ({len(missing_fields)} Felder)")
        else:
            quality = ContentQuality.INSUFFICIENT
            warnings.append(f"Unvollst√§ndig: {len(missing_fields)}/{required_count} Pflichtangaben fehlen")
        
        return quality, missing_fields, warnings

