"""
Complyo - White-Label Processor

Entfernt Third-Party-Branding und fügt Complyo-Branding ein.
Speziell für eRecht24-Texte, aber generisch verwendbar.

© 2025 Complyo.tech
"""

import re
from typing import Dict, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime


class WhiteLabelProcessor:
    """
    Verarbeitet HTML-Content und entfernt Third-Party-Branding
    """
    
    def __init__(self):
        """Initialize processor with branding patterns"""
        
        # eRecht24 Branding-Patterns (zum Entfernen)
        self.erecht24_patterns = [
            r'<[^>]*e-?recht24[^>]*>.*?</[^>]*>',  # eRecht24 tags
            r'erstellt\s+(?:mit|durch|von)\s+e-?recht24',
            r'powered\s+by\s+e-?recht24',
            r'©\s*e-?recht24',
            r'https?://(?:www\.)?e-?recht24\.de[^\s<>"\']*',
            r'e-?recht24\.de'
        ]
        
        # Generic Third-Party patterns
        self.generic_patterns = [
            r'powered\s+by\s+[\w\s]+(?=\s*[<\.])',
            r'created\s+(?:with|by)\s+[\w\s]+(?=\s*[<\.])',
            r'generated\s+by\s+[\w\s]+(?=\s*[<\.])'
        ]
        
        # Complyo Branding (zum Einfügen)
        self.complyo_branding = {
            "footer": '<p class="complyo-attribution"><small>Erstellt mit <a href="https://complyo.tech" target="_blank" rel="noopener">Complyo.tech</a> – Ihre Compliance-Plattform</small></p>',
            "meta": '<meta name="generator" content="Complyo.tech Compliance Platform">',
            "comment": f"<!-- Generated by Complyo.tech on {datetime.now().strftime('%Y-%m-%d')} -->"
        }
    
    async def process(
        self,
        html_content: str,
        options: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Hauptmethode: Verarbeitet HTML-Content
        
        Args:
            html_content: Original HTML
            options: Optional processing options
                - remove_links: bool (default True)
                - add_branding: bool (default True)
                - preserve_styles: bool (default True)
        
        Returns:
            Processed HTML
        """
        if not html_content:
            return ""
        
        # Default options
        opts = {
            "remove_links": True,
            "add_branding": True,
            "preserve_styles": True,
            "aggressive": False
        }
        if options:
            opts.update(options)
        
        # Parse HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Step 1: Remove eRecht24 branding
        soup = self._remove_erecht24_branding(soup, aggressive=opts["aggressive"])
        
        # Step 2: Remove generic third-party branding
        soup = self._remove_generic_branding(soup)
        
        # Step 3: Remove or replace external links
        if opts["remove_links"]:
            soup = self._process_external_links(soup)
        
        # Step 4: Add Complyo branding
        if opts["add_branding"]:
            soup = self._add_complyo_branding(soup)
        
        # Step 5: Clean up
        soup = self._cleanup(soup, preserve_styles=opts["preserve_styles"])
        
        # Convert back to string
        return str(soup)
    
    def _remove_erecht24_branding(
        self,
        soup: BeautifulSoup,
        aggressive: bool = False
    ) -> BeautifulSoup:
        """
        Entfernt eRecht24-spezifisches Branding
        """
        # Remove by class/id
        for pattern in ['erecht24', 'e-recht24', 'erecht', 'er24']:
            # Find by class
            for element in soup.find_all(class_=re.compile(pattern, re.IGNORECASE)):
                element.decompose()
            
            # Find by id
            for element in soup.find_all(id=re.compile(pattern, re.IGNORECASE)):
                element.decompose()
        
        # Remove by content
        for element in soup.find_all(string=re.compile(r'e-?recht24', re.IGNORECASE)):
            parent = element.parent
            
            # Check if entire parent should be removed
            if parent and parent.name in ['p', 'div', 'span', 'small']:
                text = parent.get_text().strip().lower()
                
                # Remove if it's primarily branding
                if aggressive or any(keyword in text for keyword in [
                    'erstellt mit', 'powered by', 'generated by', 
                    'created with', '© erecht24'
                ]):
                    parent.decompose()
                else:
                    # Just remove the text node
                    element.replace_with('')
        
        # Remove links to erecht24.de
        for link in soup.find_all('a', href=re.compile(r'e-?recht24\.de', re.IGNORECASE)):
            if aggressive:
                link.decompose()
            else:
                # Replace link with just the text
                link.replace_with(link.get_text())
        
        return soup
    
    def _remove_generic_branding(self, soup: BeautifulSoup) -> BeautifulSoup:
        """
        Entfernt generisches Third-Party-Branding
        """
        # Common branding phrases
        branding_phrases = [
            'powered by', 'created with', 'generated by',
            'made with', 'built with', 'erstellt mit'
        ]
        
        for phrase in branding_phrases:
            for element in soup.find_all(string=re.compile(phrase, re.IGNORECASE)):
                parent = element.parent
                
                # Check if it's a small/footer element (likely branding)
                if parent and parent.name in ['small', 'footer']:
                    parent.decompose()
                elif parent and parent.name in ['p', 'div']:
                    # Check if it's the only content
                    text = parent.get_text().strip()
                    if len(text) < 100 and phrase.lower() in text.lower():
                        parent.decompose()
        
        return soup
    
    def _process_external_links(self, soup: BeautifulSoup) -> BeautifulSoup:
        """
        Verarbeitet externe Links
        """
        for link in soup.find_all('a'):
            href = link.get('href', '')
            
            # Skip internal links and anchors
            if not href or href.startswith('#') or href.startswith('/'):
                continue
            
            # Skip mailto and tel links
            if href.startswith(('mailto:', 'tel:')):
                continue
            
            # Check if it's a third-party link (not complyo.tech)
            if 'complyo.tech' not in href.lower():
                # Option 1: Remove the link but keep text
                link.replace_with(link.get_text())
                
                # Option 2: Replace with nofollow + warning (alternative)
                # link['rel'] = 'nofollow noopener'
                # link['title'] = 'Externer Link'
        
        return soup
    
    def _add_complyo_branding(self, soup: BeautifulSoup) -> BeautifulSoup:
        """
        Fügt Complyo-Branding hinzu
        """
        # Add comment at top
        if soup.contents:
            comment = BeautifulSoup(self.complyo_branding["comment"], 'html.parser')
            soup.insert(0, comment)
        
        # Find body or last element
        body = soup.find('body')
        if not body:
            body = soup
        
        # Add footer attribution
        footer_html = BeautifulSoup(self.complyo_branding["footer"], 'html.parser')
        
        # Check if there's already a footer
        existing_footer = body.find('footer')
        if existing_footer:
            existing_footer.append(footer_html)
        else:
            # Append to body/end
            body.append(footer_html)
        
        # Add meta generator if <head> exists
        head = soup.find('head')
        if head:
            meta = BeautifulSoup(self.complyo_branding["meta"], 'html.parser')
            head.append(meta)
        
        return soup
    
    def _cleanup(
        self,
        soup: BeautifulSoup,
        preserve_styles: bool = True
    ) -> BeautifulSoup:
        """
        Final cleanup
        """
        # Remove empty tags
        for tag in soup.find_all():
            if not tag.get_text(strip=True) and not tag.name in ['br', 'hr', 'img', 'meta', 'link']:
                # Check if it has meaningful attributes
                if not any(tag.get(attr) for attr in ['id', 'class', 'src', 'href']):
                    tag.decompose()
        
        # Remove inline styles if not preserving
        if not preserve_styles:
            for tag in soup.find_all(style=True):
                del tag['style']
        
        # Remove comments (except our own)
        comments = soup.find_all(string=lambda text: isinstance(text, str) and text.strip().startswith('<!--'))
        for comment in comments:
            if 'complyo' not in comment.lower():
                comment.extract()
        
        # Normalize whitespace
        for text_node in soup.find_all(string=True):
            if text_node.parent.name not in ['script', 'style', 'pre', 'code']:
                normalized = ' '.join(text_node.split())
                text_node.replace_with(normalized)
        
        return soup
    
    def process_plain_text(
        self,
        text: str,
        add_branding: bool = True
    ) -> str:
        """
        Verarbeitet Plain-Text (nicht HTML)
        """
        # Remove eRecht24 mentions
        for pattern in self.erecht24_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # Remove generic branding
        text = re.sub(r'powered by [^\n\.]+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'erstellt (?:mit|durch) [^\n\.]+', '', text, flags=re.IGNORECASE)
        
        # Clean up multiple newlines
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Add Complyo branding
        if add_branding:
            text += f"\n\n---\nErstellt mit Complyo.tech"
        
        return text.strip()
    
    def extract_and_clean_metadata(
        self,
        html_content: str
    ) -> Dict[str, Any]:
        """
        Extrahiert und bereinigt Metadaten aus HTML
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        metadata = {
            "title": None,
            "description": None,
            "keywords": [],
            "last_updated": None,
            "version": None
        }
        
        # Extract title
        title_tag = soup.find('title')
        if title_tag:
            metadata["title"] = title_tag.get_text().strip()
        else:
            h1_tag = soup.find('h1')
            if h1_tag:
                metadata["title"] = h1_tag.get_text().strip()
        
        # Extract meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            metadata["description"] = meta_desc.get('content', '').strip()
        
        # Extract keywords
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            keywords_str = meta_keywords.get('content', '')
            metadata["keywords"] = [k.strip() for k in keywords_str.split(',') if k.strip()]
        
        # Look for last updated date
        date_patterns = [
            r'(?:stand|version|last updated|aktualisiert):\s*(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})',
            r'(\d{1,2}[\./-]\d{1,2}[\./-]\d{2,4})'
        ]
        
        text = soup.get_text()
        for pattern in date_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                metadata["last_updated"] = match.group(1)
                break
        
        return metadata
    
    def validate_processed_content(
        self,
        original: str,
        processed: str
    ) -> Dict[str, Any]:
        """
        Validiert verarbeiteten Content
        """
        soup_orig = BeautifulSoup(original, 'html.parser')
        soup_proc = BeautifulSoup(processed, 'html.parser')
        
        validation = {
            "is_valid": True,
            "warnings": [],
            "info": {}
        }
        
        # Check content length (shouldn't reduce too much)
        orig_text_len = len(soup_orig.get_text())
        proc_text_len = len(soup_proc.get_text())
        
        reduction_percent = ((orig_text_len - proc_text_len) / orig_text_len * 100) if orig_text_len > 0 else 0
        
        if reduction_percent > 20:
            validation["warnings"].append(f"Content reduced by {reduction_percent:.1f}% - verify nothing important was removed")
        
        validation["info"]["original_length"] = orig_text_len
        validation["info"]["processed_length"] = proc_text_len
        validation["info"]["reduction_percent"] = round(reduction_percent, 2)
        
        # Check if eRecht24 branding still present
        if 'erecht24' in str(soup_proc).lower() or 'e-recht24' in str(soup_proc).lower():
            validation["warnings"].append("eRecht24 branding may still be present")
        
        # Check if Complyo branding added
        if 'complyo' not in str(soup_proc).lower():
            validation["warnings"].append("Complyo branding not found")
        
        return validation


